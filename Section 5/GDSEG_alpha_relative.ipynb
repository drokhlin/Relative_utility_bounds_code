{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This notebook computes optimal portfolio weights for NYSE_2 dataset in the case of the relative power utility.\n",
    "For each alpha in alphas=[0.01,0.1,0.2,0.3,0.5,0.75] it takes an output of the GDSEG algorithm, corresponding to the largest value of the empirical utility function, obtained after n_experiments=10 experiments. \n",
    "These optimal portfolios are written to the file alpha_relative_opt_portf.txt\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(11178, 19)\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "       ahp    alcoa    amerb     coke      dow   dupont     ford       ge  \\\n0  1.01515  1.02765  1.04183  1.00637  1.00847  1.01983  1.00000  1.00000   \n1  1.01493  1.04036  0.98905  1.00475  1.00840  1.00833  1.00157  1.02187   \n2  1.00000  0.97629  0.97786  0.98583  0.99722  0.99449  0.98116  0.97860   \n3  1.02451  1.00662  1.02642  1.01917  0.99443  1.00693  1.02720  1.00795   \n4  1.03100  0.98465  1.00368  1.00313  1.02801  1.00413  1.04361  1.00394   \n\n        gm       hp      ibm    inger      jnj    kimbc    merck      mmm  \\\n0  1.01026  1.01935  1.00429  1.01357  0.99683  1.05340  1.03148  1.03377   \n1  0.99746  1.01266  0.99573  1.00446  1.00318  1.00461  1.00898  1.00251   \n2  0.98219  0.98125  0.98571  0.99556  0.95873  0.98165  0.98043  0.95990   \n3  0.98705  1.00637  1.01522  1.00000  1.01325  0.98131  1.01089  1.03655   \n4  1.00525  1.03165  1.02427  1.01563  1.00654  1.02381  1.01077  0.99496   \n\n    morris    pandg   schlum  \n0  1.01495  1.00775  1.01176  \n1  1.00000  1.00192  1.01938  \n2  0.97218  0.98656  0.97338  \n3  0.99663  1.00778  1.00000  \n4  0.98649  1.01158  1.01563  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ahp</th>\n      <th>alcoa</th>\n      <th>amerb</th>\n      <th>coke</th>\n      <th>dow</th>\n      <th>dupont</th>\n      <th>ford</th>\n      <th>ge</th>\n      <th>gm</th>\n      <th>hp</th>\n      <th>ibm</th>\n      <th>inger</th>\n      <th>jnj</th>\n      <th>kimbc</th>\n      <th>merck</th>\n      <th>mmm</th>\n      <th>morris</th>\n      <th>pandg</th>\n      <th>schlum</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.01515</td>\n      <td>1.02765</td>\n      <td>1.04183</td>\n      <td>1.00637</td>\n      <td>1.00847</td>\n      <td>1.01983</td>\n      <td>1.00000</td>\n      <td>1.00000</td>\n      <td>1.01026</td>\n      <td>1.01935</td>\n      <td>1.00429</td>\n      <td>1.01357</td>\n      <td>0.99683</td>\n      <td>1.05340</td>\n      <td>1.03148</td>\n      <td>1.03377</td>\n      <td>1.01495</td>\n      <td>1.00775</td>\n      <td>1.01176</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.01493</td>\n      <td>1.04036</td>\n      <td>0.98905</td>\n      <td>1.00475</td>\n      <td>1.00840</td>\n      <td>1.00833</td>\n      <td>1.00157</td>\n      <td>1.02187</td>\n      <td>0.99746</td>\n      <td>1.01266</td>\n      <td>0.99573</td>\n      <td>1.00446</td>\n      <td>1.00318</td>\n      <td>1.00461</td>\n      <td>1.00898</td>\n      <td>1.00251</td>\n      <td>1.00000</td>\n      <td>1.00192</td>\n      <td>1.01938</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.00000</td>\n      <td>0.97629</td>\n      <td>0.97786</td>\n      <td>0.98583</td>\n      <td>0.99722</td>\n      <td>0.99449</td>\n      <td>0.98116</td>\n      <td>0.97860</td>\n      <td>0.98219</td>\n      <td>0.98125</td>\n      <td>0.98571</td>\n      <td>0.99556</td>\n      <td>0.95873</td>\n      <td>0.98165</td>\n      <td>0.98043</td>\n      <td>0.95990</td>\n      <td>0.97218</td>\n      <td>0.98656</td>\n      <td>0.97338</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.02451</td>\n      <td>1.00662</td>\n      <td>1.02642</td>\n      <td>1.01917</td>\n      <td>0.99443</td>\n      <td>1.00693</td>\n      <td>1.02720</td>\n      <td>1.00795</td>\n      <td>0.98705</td>\n      <td>1.00637</td>\n      <td>1.01522</td>\n      <td>1.00000</td>\n      <td>1.01325</td>\n      <td>0.98131</td>\n      <td>1.01089</td>\n      <td>1.03655</td>\n      <td>0.99663</td>\n      <td>1.00778</td>\n      <td>1.00000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.03100</td>\n      <td>0.98465</td>\n      <td>1.00368</td>\n      <td>1.00313</td>\n      <td>1.02801</td>\n      <td>1.00413</td>\n      <td>1.04361</td>\n      <td>1.00394</td>\n      <td>1.00525</td>\n      <td>1.03165</td>\n      <td>1.02427</td>\n      <td>1.01563</td>\n      <td>1.00654</td>\n      <td>1.02381</td>\n      <td>1.01077</td>\n      <td>0.99496</td>\n      <td>0.98649</td>\n      <td>1.01158</td>\n      <td>1.01563</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "# Importing NYSE_2 dataset\n",
    "stocks=pd.read_csv('NYSE_2.csv')\n",
    "print(stocks.shape)\n",
    "stocks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r: array for stock returns\n",
    "N=stocks.shape[0]\n",
    "d=stocks.shape[1]\n",
    "r=np.zeros((N,d))\n",
    "r=stocks.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GDSEG(U,R,n_attempts=10**4,threshold=1/10**10):\n",
    "    \"\"\" Greedy doubly stochastic exponentiated graient algorithm (GDSEG)\n",
    "    U: empirical utility, power or logarithmic \n",
    "    R: array of stock pirces\n",
    "    The function returns the optimal oprtfolio weights: w_old, the optimal value of the objective function: U(w_old,R), and the number of iterations: i\n",
    "    \"\"\"\n",
    "    N=R.shape[0]\n",
    "    d=R.shape[1]\n",
    "    w_old=np.ones(d)/d\n",
    "    w_new=np.zeros(d)\n",
    "    attempt=0\n",
    "    i=0\n",
    "    while attempt<=n_attempts:\n",
    "        i+=1\n",
    "        k=np.random.randint(0,N)\n",
    "        eta=np.random.rand()\n",
    "        a=[w_old[j]*np.exp(eta*R[k,j]/(np.dot(w_old,R[k,:]))**(1-alpha)) for j in range(d)]\n",
    "        w_new=a/np.sum(a)\n",
    "        attempt+=1\n",
    "        if U(w_new,R)>U(w_old,R)+threshold:\n",
    "            w_old=w_new\n",
    "            attempt=0\n",
    "        #if i%10000==0: print(s,i,100*w_old,U(w_old,R),eta)\n",
    "    return w_old, U(w_old,R), i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def U(nu,r):\n",
    "    \"\"\" Empirical utility, power (0<alpha<=1) or logarithmic (alpha=0) \"\"\"\n",
    "    if alpha==0:\n",
    "        return np.mean(np.log(np.dot(r[0:N,:],nu)))\n",
    "    else:\n",
    "        return np.mean(np.dot(r[0:N,:],nu)**alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "alpha: 0.01 GDSEG experiment: 0 Number of iterations: 55436.0\nalpha: 0.01 GDSEG experiment: 1 Number of iterations: 44634.0\nalpha: 0.01 GDSEG experiment: 2 Number of iterations: 51797.0\nalpha: 0.01 GDSEG experiment: 3 Number of iterations: 31390.0\nalpha: 0.01 GDSEG experiment: 4 Number of iterations: 69557.0\nalpha: 0.01 GDSEG experiment: 5 Number of iterations: 54171.0\nalpha: 0.01 GDSEG experiment: 6 Number of iterations: 40173.0\nalpha: 0.01 GDSEG experiment: 7 Number of iterations: 35287.0\nalpha: 0.01 GDSEG experiment: 8 Number of iterations: 59475.0\nalpha: 0.01 GDSEG experiment: 9 Number of iterations: 44767.0\nalpha: 0.1 GDSEG experiment: 0 Number of iterations: 41852.0\nalpha: 0.1 GDSEG experiment: 1 Number of iterations: 33579.0\nalpha: 0.1 GDSEG experiment: 2 Number of iterations: 40338.0\nalpha: 0.1 GDSEG experiment: 3 Number of iterations: 73189.0\nalpha: 0.1 GDSEG experiment: 4 Number of iterations: 58481.0\nalpha: 0.1 GDSEG experiment: 5 Number of iterations: 73921.0\nalpha: 0.1 GDSEG experiment: 6 Number of iterations: 36512.0\nalpha: 0.1 GDSEG experiment: 7 Number of iterations: 81465.0\nalpha: 0.1 GDSEG experiment: 8 Number of iterations: 35584.0\nalpha: 0.1 GDSEG experiment: 9 Number of iterations: 51568.0\nalpha: 0.2 GDSEG experiment: 0 Number of iterations: 55835.0\nalpha: 0.2 GDSEG experiment: 1 Number of iterations: 43346.0\nalpha: 0.2 GDSEG experiment: 2 Number of iterations: 40338.0\nalpha: 0.2 GDSEG experiment: 3 Number of iterations: 49713.0\nalpha: 0.2 GDSEG experiment: 4 Number of iterations: 55693.0\nalpha: 0.2 GDSEG experiment: 5 Number of iterations: 44588.0\nalpha: 0.2 GDSEG experiment: 6 Number of iterations: 40173.0\nalpha: 0.2 GDSEG experiment: 7 Number of iterations: 39633.0\nalpha: 0.2 GDSEG experiment: 8 Number of iterations: 37823.0\nalpha: 0.2 GDSEG experiment: 9 Number of iterations: 61095.0\nalpha: 0.3 GDSEG experiment: 0 Number of iterations: 63295.0\nalpha: 0.3 GDSEG experiment: 1 Number of iterations: 43346.0\nalpha: 0.3 GDSEG experiment: 2 Number of iterations: 40338.0\nalpha: 0.3 GDSEG experiment: 3 Number of iterations: 49713.0\nalpha: 0.3 GDSEG experiment: 4 Number of iterations: 55693.0\nalpha: 0.3 GDSEG experiment: 5 Number of iterations: 36524.0\nalpha: 0.3 GDSEG experiment: 6 Number of iterations: 40173.0\nalpha: 0.3 GDSEG experiment: 7 Number of iterations: 39633.0\nalpha: 0.3 GDSEG experiment: 8 Number of iterations: 59475.0\nalpha: 0.3 GDSEG experiment: 9 Number of iterations: 61095.0\nalpha: 0.5 GDSEG experiment: 0 Number of iterations: 42551.0\nalpha: 0.5 GDSEG experiment: 1 Number of iterations: 33226.0\nalpha: 0.5 GDSEG experiment: 2 Number of iterations: 52888.0\nalpha: 0.5 GDSEG experiment: 3 Number of iterations: 40522.0\nalpha: 0.5 GDSEG experiment: 4 Number of iterations: 44143.0\nalpha: 0.5 GDSEG experiment: 5 Number of iterations: 47258.0\nalpha: 0.5 GDSEG experiment: 6 Number of iterations: 34928.0\nalpha: 0.5 GDSEG experiment: 7 Number of iterations: 40352.0\nalpha: 0.5 GDSEG experiment: 8 Number of iterations: 37420.0\nalpha: 0.5 GDSEG experiment: 9 Number of iterations: 39391.0\nalpha: 0.75 GDSEG experiment: 0 Number of iterations: 51732.0\nalpha: 0.75 GDSEG experiment: 1 Number of iterations: 43055.0\nalpha: 0.75 GDSEG experiment: 2 Number of iterations: 52888.0\nalpha: 0.75 GDSEG experiment: 3 Number of iterations: 40522.0\nalpha: 0.75 GDSEG experiment: 4 Number of iterations: 44143.0\nalpha: 0.75 GDSEG experiment: 5 Number of iterations: 47258.0\nalpha: 0.75 GDSEG experiment: 6 Number of iterations: 35961.0\nalpha: 0.75 GDSEG experiment: 7 Number of iterations: 40352.0\nalpha: 0.75 GDSEG experiment: 8 Number of iterations: 37420.0\nalpha: 0.75 GDSEG experiment: 9 Number of iterations: 39391.0\n"
    }
   ],
   "source": [
    "# Computation of the optimal portfolios\n",
    "n_experiments=10\n",
    "alphas=[0.01,0.1,0.2,0.3,0.5,0.75]\n",
    "opt_portf=np.zeros((n_experiments,d))\n",
    "opt_portf_best=np.zeros((len(alphas),d))\n",
    "opt_val=np.zeros(n_experiments)\n",
    "n_iter=np.zeros(n_experiments)\n",
    "r_rel=np.zeros((N,d))\n",
    "f=open('alpha_relative_opt_portf.txt','ab')\n",
    "al_num=-1\n",
    "for t in range(N):\n",
    "    r_rel[t,:]=r[t,:]/np.max(r[t,:]) \n",
    "for alpha in alphas:\n",
    "    al_num+=1\n",
    "    for s in range(n_experiments):\n",
    "        np.random.seed(1+s)\n",
    "        opt_portf[s,:], opt_val[s], n_iter[s]  = GDSEG(U,r_rel)\n",
    "        print('alpha:',alpha,'GDSEG experiment:',s,'Number of iterations:',n_iter[s])\n",
    "    opt_portf_best[al_num,:]=opt_portf[np.argmax(opt_val),:]\n",
    "    np.savetxt(f,opt_portf_best[al_num,:].reshape(1,d))\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}